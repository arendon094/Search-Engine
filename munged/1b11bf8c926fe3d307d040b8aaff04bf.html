<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en" dir="ltr">
 <head> 
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> 
  <!-- headlinks removed --> 
  <link rel="shortcut icon" href="../../../../misc/favicon.ico"> 
  <title>Maximum entropy thermodynamics - Wikipedia, the free encyclopedia</title> 
  <style type="text/css">/*<![CDATA[*/ @import "../../../../skins/offline/main.css"; /*]]>*/</style> 
  <link rel="stylesheet" type="text/css" media="print" href="../../../../skins/common/commonPrint.css"> 
  <!--[if lt IE 5.5000]><style type="text/css">@import "../../../../skins/monobook/IE50Fixes.css";</style><![endif]--> 
  <!--[if IE 5.5000]><style type="text/css">@import "../../../../skins/monobook/IE55Fixes.css";</style><![endif]--> 
  <!--[if IE 6]><style type="text/css">@import "../../../../skins/monobook/IE60Fixes.css";</style><![endif]--> 
  <!--[if IE]><script type="text/javascript" src="../../../../skins/common/IEFixes.js"></script>
    <meta http-equiv="imagetoolbar" content="no" /><![endif]--> 
  <script type="text/javascript" src="../../../../skins/common/wikibits.js"></script> 
  <script type="text/javascript" src="../../../../skins/offline/md5.js"></script> 
  <script type="text/javascript" src="../../../../skins/offline/utf8.js"></script> 
  <script type="text/javascript" src="../../../../skins/offline/lookup.js"></script> 
  <script type="text/javascript" src="../../../../raw/gen.js"></script> 
  <style type="text/css">/*<![CDATA[*/
@import "../../../../raw/MediaWiki%7ECommon.css";
@import "../../../../raw/MediaWiki%7EMonobook.css";
@import "../../../../raw/gen.css";
/*]]>*/</style> 
 </head> 
 <body class="ns-0"> 
  <div id="globalWrapper"> 
   <div id="column-content"> 
    <div id="content"> 
     <a name="top" id="contentTop"></a> 
     <h1 class="firstHeading">Maximum entropy thermodynamics</h1> 
     <div id="bodyContent"> 
      <h3 id="siteSub">From Wikipedia, the free encyclopedia</h3> 
      <div id="contentSub"></div> 
      <!-- start content --> 
      <p>In <a href="831facf27734eba569d62ab6a7e6c903.html" title="Physics">physics</a> the <b>Maximum entropy school of thermodynamics</b> (or more colloquially, the <i>MaxEnt</i> school of thermodynamics), initiated with two papers published in the Physical Review by <a href="1420c223ade4fdd27a84609ff0cf39d7.html" class="mw-redirect" title="Edwin T. Jaynes">Edwin T. Jaynes</a> in <a href="cd63622067772ed12ec9a1bab178f3c1.html" title="1957">1957</a>, views <a href="ec0236a618d6a1b35d346e01f784c7cb.html" title="Statistical mechanics">statistical mechanics</a> as an <a href="285220c02cd1b0864eb022aaa6794a70.html" title="Inference">inference</a> process: a specific application of inference techniques rooted in <a href="9477c27df487086eddc9e469910b0d51.html" title="Information theory">information theory</a>, which relate not just to <a href="75b304519ecd4e7cc42a6cf6ea51cac4.html" title="Equilibrium thermodynamics">equilibrium thermodynamics</a>, but are general to all problems requiring prediction from incomplete or insufficient data (such as for example <a href="7c649666c2d4ecf42a18f83f41330c1d.html" title="Image processing">image reconstruction</a>, <a href="68296bd0b32b3a9944ff3b12635685d3.html" title="Spectral analysis">spectral analysis</a>, or <a href="354cd3c050c09192f084f1291bd3c463.html" title="Inverse problem">inverse problems</a>).</p> 
      <table id="toc" class="toc" summary="Contents"> 
       <tbody>
        <tr> 
         <td> 
          <div id="toctitle"> 
           <h2>Contents</h2> 
          </div> 
          <ul> 
           <li class="toclevel-1"><a href="3afee52ec79253e27dc5aaa73dce0d4f.html"><span class="tocnumber">1</span> <span class="toctext">Maximum Shannon entropy</span></a></li> 
           <li class="toclevel-1"><a href="ff1cec29392662384661822d58db6dbd.html"><span class="tocnumber">2</span> <span class="toctext">Philosophical Implications</span></a> 
            <ul> 
             <li class="toclevel-2"><a href="5ea019666ffcb75cd504261082e42a4f.html"><span class="tocnumber">2.1</span> <span class="toctext">The nature of the probabilities in statistical mechanics</span></a></li> 
             <li class="toclevel-2"><a href="09d47f4926ed54a44e9b6030e0ed3c71.html"><span class="tocnumber">2.2</span> <span class="toctext">Is entropy "real"&nbsp;?</span></a></li> 
             <li class="toclevel-2"><a href="c452a31418fe4e90775a56d0a6c2d825.html"><span class="tocnumber">2.3</span> <span class="toctext">Is ergodic theory relevant&nbsp;?</span></a></li> 
             <li class="toclevel-2"><a href="c9346932d3b8ec09a70e0a4494093611.html"><span class="tocnumber">2.4</span> <span class="toctext">The Second Law</span></a></li> 
             <li class="toclevel-2"><a href="3d467d5b1d48a22a9f58597f41bc6f17.html"><span class="tocnumber">2.5</span> <span class="toctext">Caveats with the argument</span></a></li> 
            </ul> </li> 
           <li class="toclevel-1"><a href="69e663a9b6be0652c4e7dcf22ac3782f.html"><span class="tocnumber">3</span> <span class="toctext">References</span></a></li> 
          </ul> </td> 
        </tr> 
       </tbody>
      </table> 
      <script type="text/javascript">
//<![CDATA[
 if (window.showTocToggle) { var tocShowText = "show"; var tocHideText = "hide"; showTocToggle(); } 
//]]>
</script> 
      <p><a name="Maximum_Shannon_entropy" id="Maximum_Shannon_entropy"></a></p> 
      <h2><span class="editsection">[<a href="5e929c72016d6a67b2432043e84c5472.html" title="Edit section: Maximum Shannon entropy">edit</a>]</span> <span class="mw-headline">Maximum Shannon entropy</span></h2> 
      <p>Central to the MaxEnt thesis is the <a href="06fc251920bc01660f76cf2cb80e131c.html" title="Principle of maximum entropy">principle of maximum entropy</a>, which states that given certain <i>testable information</i> about a probability distribution, for example particular expectation values, but which is not in itself sufficient to uniquely determine the distribution, one should prefer the distribution which maximises the <a href="0f41cc86ee7ba05b3f5986b9b8ecaba0.html" class="mw-redirect" title="Shannon entropy">Shannon information entropy</a>.</p> 
      <dl> 
       <dd>
        <img class="tex" alt="S_I = - \sum p_i \ln p_i" src="../../../../math/9/7/1/971c7e3571826ae21898eb45b3c13705.png">
       </dd> 
      </dl> 
      <p>This is known as the <a href="f977f5a803a534bdc1f772ff04a650b3.html" title="Gibbs algorithm">Gibbs algorithm</a>, having been introduced first by <a href="922bce3408217e266bf840f251d9be7c.html" class="mw-redirect" title="J. Willard Gibbs">J. Willard Gibbs</a> in <a href="22c69d6fa84f7bbace15f5e4723060d4.html" title="1878">1878</a>, to set up <a href="b35783dc26506a61934ffb107fbd8a5c.html" class="mw-redirect" title="Statistical ensemble">statistical ensembles</a> to predict the properties of thermodynamic systems at equilibrium. It is the cornerstone of the statistical mechanical analysis of the thermodynamic properties of equilibrium systems. (See <i><a href="3411cf2a9c6b7690ce29463acc65da30.html" title="Partition function">partition function</a></i>).</p> 
      <p>A direct connection is thus made between the equilibrium <a href="bbbf53425cbd913dc927410722ced44e.html" class="mw-redirect" title="Thermodynamic entropy">thermodynamic entropy</a> <i>S<sub>Th</sub></i>, a <a href="78cd6ae19a63234678203bdba42b947f.html" title="State function">state function</a> of pressure, volume, temperature, etc., and the <a href="61c37d47bea9219aa4308c0cf7975e4e.html" title="Information entropy">information entropy</a> for the predicted distribution with maximum uncertainty conditioned only on the expectation values of those variables:</p> 
      <dl> 
       <dd>
        <img class="tex" alt="S_{Th}(P,V,T,...)_{(eqm)} = k_B \, S_I(P,V,T,...)" src="../../../../math/3/1/4/3148fe92f6d48f02f2e32ea637433b8a.png">
       </dd> 
      </dl> 
      <p>The presence of <i>k<sub>B</sub></i>, Boltzmann's constant, has no fundamental physical significance here, but is necessary to retain consistency with the previous historical definition of entropy by <a href="42b00a6c91b5c0931fa446e171be4573.html" class="mw-redirect" title="Clausius">Clausius</a> (<a href="b37ca9b772e5303bd71dc7ff4ca59781.html" title="1865">1865</a>). (For further discussion, see <i><a href="08cf2816cdd1256422c3cb833c8e46d7.html" class="mw-redirect" title="Boltzmann's constant">Boltzmann's constant</a></i>).</p> 
      <p>However, the MaxEnt school argue that the MaxEnt approach is a general technique of statistical inference, with applications far beyond this. It can therefore also be used to predict a distribution for <i>trajectories</i> Î“ <i>over a period of time</i>, by maximising:</p> 
      <dl> 
       <dd>
        <img class="tex" alt="S_I = - \sum p_{\Gamma} \ln p_{\Gamma}" src="../../../../math/d/9/0/d90c7a3d34ddf23f9e8a9fc3c1a702e8.png">
       </dd> 
      </dl> 
      <p>This <i>information entropy</i> does <i>not</i> necessarily have a simple correspondence with thermodynamic entropy; but it can be used to predict features of <a href="9b6c009ee10efea266e3db679601f457.html" class="mw-redirect" title="Nonequilibrium thermodynamics">non-equilibrium</a> thermodynamic systems as they evolve over time.</p> 
      <p>In the field of near-equilibrium thermodynamics, the <a href="8eb6b902c3dd4e300dd810fe05c2098f.html" title="Onsager reciprocal relations">Onsager reciprocal relations</a> and the <a href="77df5937545c8c9786f15833e89f2404.html" class="mw-redirect" title="Green-Kubo relations">Green-Kubo relations</a> fall out very directly. The approach also creates a solid theoretical framework for the study of far-from-equilibrium thermodynamics, making the derivation of the <a href="36f060f1ba74061323ed962e842ce072.html" title="Fluctuation theorem">entropy production fluctuation theorem</a> particularly straightforward. However, practical calculations for most far-from-equilibrium systems remain very challenging.</p> 
      <p>(Technical note: for the reasons discussed in the article <i><a href="754d3f4fc1cc20f4de1e1e24fee4bb68.html" title="Differential entropy">differential entropy</a></i>, the simple definition of Shannon entropy ceases to be so directly applicable for probabilities of continuous variables. Instead the appropriate quantity to maximise is the <i>relative information entropy</i>,</p> 
      <dl> 
       <dd>
        <img class="tex" alt="H_c=-\int p(x)\log\frac{p(x)}{m(x)}\,dx," src="../../../../math/2/8/2/2825a4a154860c33df38d08883eaaad6.png">
       </dd> 
      </dl> 
      <p>the negative of the <a href="dd56384df3be3cff1d3d12c8729951aa.html" class="mw-redirect" title="Kullback-Leibler divergence">Kullback-Leibler divergence</a>, or discrimination information, of <i>m</i>(<i>x</i>) from <i>p</i>(<i>x</i>), where <i>m</i>(<i>x</i>) is a prior invariant measure for the variable(s). The relative entropy <i>H<sub>c</sub></i> is always less than zero, and can be thought of as (the negative of) the number of <a href="053c7a12038b17f6ae7dbb87363a6e6b.html" title="Bit">bits</a> of uncertainty lost by fixing on <i>p</i>(<i>x</i>) rather than <i>m</i>(<i>x</i>). Unlike the Shannon entropy, the relative entropy <i>H<sub>c</sub></i> has the advantage of remaining finite and well-defined for continuous <i>x</i>, and invariant under 1-to-1 co-ordinate transformations. The two expressions co-incide for discrete probability distributions, if one can make the assumption that <i>m</i>(<i>x</i><sub>i</sub>) is uniform - i.e. the <a href="4ba15020116af97a89b564f771100d1b.html" class="mw-redirect" title="Principle of equal a-priori probability">principle of equal a-priori probability</a>, which underlies statistical thermodynamics).</p> 
      <p><a name="Philosophical_Implications" id="Philosophical_Implications"></a></p> 
      <h2><span class="editsection">[<a href="dbd12d9788c8b4195cdef90bc2e67bc2.html" title="Edit section: Philosophical Implications">edit</a>]</span> <span class="mw-headline">Philosophical Implications</span></h2> 
      <p>Adherents to the MaxEnt viewpoint tend to take a very definite position on some of the <a href="fd3ce82a7f92972608b01da1aa35d9c7.html" title="Philosophy of thermal and statistical physics">conceptual/philosophical questions</a> in thermodynamics.</p> 
      <p><a name="The_nature_of_the_probabilities_in_statistical_mechanics" id="The_nature_of_the_probabilities_in_statistical_mechanics"></a></p> 
      <h3><span class="editsection">[<a href="e5c351818d54b7118f9ab11842c0bef4.html" title="Edit section: The nature of the probabilities in statistical mechanics">edit</a>]</span> <span class="mw-headline">The nature of the probabilities in statistical mechanics</span></h3> 
      <p>According to the MaxEnt viewpoint, the probabilities in statistical mechanics are <i><a href="3e8e65ff0bb6ea64b4e992c8f6ffe460.html" title="Subjectivity">subjective</a></i> (epistemic, personal), to the extent that they are conditioned on a particular model for the underlying state space (e.g. Liouvillian <a href="43da9c20d4d47ed709c1f0fd1d626917.html" title="Phase space">phase space</a>); and they are conditioned on a particular partial description of the system (the macroscopic description of the system used to constrain the MaxEnt probability assignment). The probabilities are <i><a href="f89ea2b9517bee9202835251a9a9d3f5.html" title="Objectivity (science)">objective</a></i> to the extent that given these inputs, a uniquely defined probability distribution will result.</p> 
      <p>At a trivial level, the probabilities cannot be entirely objective, because in reality there is only one system, and (assuming <a href="deeb9440353f9a71a05386b04b27cf2d.html" title="Determinism">determinism</a>) a single unknown trajectory it will evolve through. The probabilities therefore represent a lack of information in the analyst's macroscopic description of the system, not a property of the underlying reality itself.</p> 
      <p>Moreover, the quality of the predicted probabilities depends on whether the macroscopic model constraints really are a sufficiently accurate and/or complete description of the system to capture all of the experimentally reproducible behaviour. This cannot be guaranteed, <i>a priori</i>. For this reason MaxEnt proponents also call the method <b>predictive statistical mechanics</b>. The predictions can fail. But if they do, this is informative, because it signals the presence of new constraints needed to capture reproducible behaviour in the system, which had not been taken into account.</p> 
      <p><a name="Is_entropy_.22real.22_.3F" id="Is_entropy_.22real.22_.3F"></a></p> 
      <h3><span class="editsection">[<a href="d65f9e14fd9abdfa6129eaa9354eec6d.html" title="Edit section: Is entropy &quot;real&quot;&nbsp;?">edit</a>]</span> <span class="mw-headline">Is entropy "real"&nbsp;?</span></h3> 
      <p>The thermodynamic entropy (at equilibrium) is a function of the state variables of the model description. It is therefore as "real" as the other variables in the model description. If the model constraints in the probability assignment are a "good" description, containing all the information needed to predict reproducible experimental results, then that includes all of the results one could predict using the formulae involving entropy from classical thermodynamics. To that extent, the MaxEnt <i>S<sub>Th</sub></i> is as "real" as the entropy in classical thermodynamics.</p> 
      <p>Of course in reality there is only one real state of the system. The entropy is not a direct function of that state. It is a function of the real state only through the (subjectively chosen) macroscopic model description.</p> 
      <p><a name="Is_ergodic_theory_relevant_.3F" id="Is_ergodic_theory_relevant_.3F"></a></p> 
      <h3><span class="editsection">[<a href="3e6dff0d03c7bf4a5a03983f2950e487.html" title="Edit section: Is ergodic theory relevant&nbsp;?">edit</a>]</span> <span class="mw-headline">Is ergodic theory relevant&nbsp;?</span></h3> 
      <p>The Gibbsian <a href="fa8e4e0009db7943bd011eee9886205a.html" class="mw-redirect" title="Statistical ensemble">ensemble</a> idealises the notion of repeating an experiment again and again on <i>different</i> systems, not again and again on the <i>same</i> system. So long-term time averages and the <a href="6c1fbceb0c0149c762e391827f20b89f.html" title="Ergodic hypothesis">ergodic hypothesis</a>, despite the intense interest in them in the first part of the <a href="2c44db59477efefc9f8bc417d48b6d58.html" class="mw-redirect" title="Twentieth century">twentieth century</a>, strictly speaking are not relevant to the probability assignment for the state one might find the system in.</p> 
      <p>However, this changes if there is additional knowledge that the system is being prepared in a particular way some time before the measurement. One must then consider whether this gives further information which is still relevant at the time of measurement. The question of how 'rapidly mixing' different properties of the system are then becomes very much of interest. Information about some degrees of freedom of the combined system may become unusable very quickly; information about other properties of the system may go on being relevant for a considerable time.</p> 
      <p>If nothing else, the medium and long-run time correlation properties of the system are interesting subjects for experimentation in themselves. Failure to accurately predict them is a good indicator that relevant macroscopically determinable physics may be missing from the model.</p> 
      <p><a name="The_Second_Law" id="The_Second_Law"></a></p> 
      <h3><span class="editsection">[<a href="9ddfc985b54195c387ccad105eec2265.html" title="Edit section: The Second Law">edit</a>]</span> <span class="mw-headline">The Second Law</span></h3> 
      <p>According to <a href="8f175cdeb1aebc9dc0698716d9da9b36.html" title="Liouville's theorem (Hamiltonian)">Liouville's theorem</a> for <a href="d70ba70e44ffa4b87a9daaab22701a68.html" class="mw-redirect" title="Hamiltonian dynamics">Hamiltonian dynamics</a>, the hyper-volume of a cloud of points in <a href="ef57ebb86175b7328e5256f2c883f7e1.html" title="Phase space">phase space</a> remains constant as the system evolves. Therefore, the information entropy must also remain constant, if we condition on the original information, and then follow each of those microstates forward in time:</p> 
      <dl> 
       <dd>
        <img class="tex" alt="\Delta S_I = 0 \," src="../../../../math/0/b/2/0b29810ea6fc4dd31f7b787d2f6294e4.png">
       </dd> 
      </dl> 
      <p>However, as time evolves, that initial information we had becomes less directly accessible. Instead of being easily summarisable in the macroscopic description of the system, it increasingly relates to very subtle correlations between the positions and momenta of individual molecules. (Compare the discussion of Boltzmann's <a href="cbbfcd1064fc95d54c1b6a42e9746c96.html" title="H-theorem">H-theorem</a>). Equivalently, it means that the probability distribution for the whole system, in 6N-dimensional phase space, becomes increasingly irregular, spreading out into long thin fingers rather than the initial tightly defined volume of possibilities.</p> 
      <p>Classical thermodynamics is built on the assumption that entropy is a <i>state function</i> of the macroscopic variables -- ie that none of the history of the system matters, it can all be ignored.</p> 
      <p>The extended, wispy, evolved probability distribution, which still has the initial Shannon entropy <i>S<sub>Th</sub><sup>(1)</sup></i>, should reproduce the expectation values of the observed macroscopic variables at time <i>t<sub>2</sub></i>. However it will no longer necessarily be a maximum entropy distribution for that new macroscopic description. On the other hand, the new thermodynamic entropy <i>S<sub>Th</sub><sup>(2)</sup></i> assuredly <i>will</i> measure the maximum entropy distribution, by construction. Therefore, we expect:</p> 
      <dl> 
       <dd>
        <img class="tex" alt="{S_{Th}}^{(2)} \geq {S_{Th}}^{(1)} " src="../../../../math/2/d/5/2d518f988504354c33ff2cb6c6285a4d.png">
       </dd> 
      </dl> 
      <p>This result can be interpreted at different levels. At an abstract level, it means simply that some of the information we originally had about the system has become <i>no longer useful</i> at a macroscopic level. Alternatively, at the level of the 6N-dimensional probability distribution, it represents <a href="df2a1d4561f6baf509ecda5da1b17d2d.html" class="mw-redirect" title="Coarse graining">coarse graining</a> -- ie information loss by smoothing out very fine-scale detail.</p> 
      <p><a name="Caveats_with_the_argument" id="Caveats_with_the_argument"></a></p> 
      <h3><span class="editsection">[<a href="a37f85153892200f3df24fdf790e01d8.html" title="Edit section: Caveats with the argument">edit</a>]</span> <span class="mw-headline">Caveats with the argument</span></h3> 
      <p>Some caveats should be considered with the above.</p> 
      <p>1. Like all statistical mechanical results according to the MaxEnt school, this increase in thermodynamic entropy is only a <i>prediction</i>. It assumes in particular that the initial macroscopic description contains all of the information relevant to predicting the later macroscopic state. This may not be the case, for example if the initial description fails to reflect some aspect of the preparation of the system which later becomes relevant. In that case the <i>failure</i> of a MaxEnt prediction tells us that there is something more which is relevant that we may have overlooked in the physics of the system.</p> 
      <p>It is also sometimes suggested that <a href="47a9a34975aa0c1a2489bfe20fa964ca.html" class="mw-redirect" title="Quantum measurement">quantum measurement</a>, especially in the <a href="ad25bc8c4853e15645ab1793ff18de22.html" class="mw-redirect" title="Decoherence">decoherence</a> interpretation, may give an apparently unexpected reduction in entropy per this argument, as it appears to involve macroscopic information becoming available which was previously inaccessible. (However, the entropy accounting of quantum measurement is tricky, because to get full decoherence one may be assuming an infinite environment, with an infinite entropy).</p> 
      <p>2. The argument so far has glossed over the question of <i>fluctuations</i>. It has also implicitly assumed that the uncertainty predicted at time <i>t<sub>1</sub></i> for the variables at time <i>t<sub>2</sub></i> will be much smaller than the measurement error. But if the measurements do meaningfully update our knowledge of the system, our uncertainty as to its state is reduced, giving a new <i>S<sub><b>I</b></sub><sup>(2)</sup></i> which is <i>less</i> than <i>S<sub><b>I</b></sub><sup>(1)</sup></i>. (Note that if we allow ourselves the abilities of <a href="d68523dd3f5973c48277e6584c34fe16.html" title="Laplace's demon">Laplace's demon</a>, the consequences of this new information can also be mapped backwards, so our uncertainty about the dynamical state at time <i>t<sub>1</sub></i> is now <i>also</i> reduced from <i>S<sub><b>I</b></sub><sup>(1)</sup></i> to <i>S<sub><b>I</b></sub><sup>(2)</sup></i>&nbsp;).</p> 
      <p>We know that <i>S<sub>Th</sub><sup>(2)</sup> &gt; S<sub><b>I</b></sub><sup>(2)</sup></i>; but we can now no longer be certain that it is greater than <i>S<sub>Th</sub><sup>(1)</sup> = S<sub><b>I</b></sub><sup>(1)</sup></i>. This then leaves open the possibility for fluctuations in <i>S<sub>Th</sub></i>. The thermodynamic entropy may go <i>down</i> as well as up. A more sophisticated analysis is given by the entropy <a href="33bc771c59c5612b7ada778194019329.html" class="mw-redirect" title="Fluctuation Theorem">Fluctuation Theorem</a>, which can be established as a consequence of the time-dependent MaxEnt picture.</p> 
      <p>3. As just indicated, the MaxEnt inference runs equally well in reverse. So given a particular final state, we can ask, what can we <i>retrodict</i> to improve our knowledge about earlier states? However the Second Law argument above also runs in reverse: given macroscopic information at time <i>t<sub>2</sub></i>, we should expect it too to become less useful. The two procedures are time-symmetric. But now the information will become less and less useful at earlier and earlier times. (Compare <i><a href="7c7802ca72d75afb289084935e13ab5c.html" title="Loschmidt's paradox">Loschmidt's paradox</a></i>). The MaxEnt inference would predict that the most probable origin of a currently low-entropy state would be as a spontaneous fluctuation from an earlier high entropy state. But this conflicts with what we know to have happened, namely that entropy has been increasing steadily, even back in the past.</p> 
      <p>The MaxEnt proponents' response to this would be that such a systematic failing in the prediction of a MaxEnt inference is a <i>good</i> thing. It means that there is thus clear evidence that some important physical information has been missed in the specification the problem. If it is correct that the dynamics <i>are</i> <a href="0826a25528cf4c48267b8cdc0dc3993d.html" title="T-symmetry">time-symmetric</a>, it appears that we need to put in by hand a <a href="94d98f50af0a8557a650a6d418d433da.html" title="Prior probability">prior probability</a> that initial configurations with a low thermodynamic entropy are more likely than initial configurations with a high thermodynamic entropy. This cannot be explained by the immediate dynamics. Quite possibly it arises as a reflection of the evident time-asymmetric evolution of the universe on a cosmological scale. (See article: <i><a href="40648e33d9f340c21e0001782d8973d3.html" title="Arrow of time">Arrow of time</a></i>).</p> 
      <p><a name="References" id="References"></a></p> 
      <h2><span class="editsection">[<a href="735689e81ee81993ee90d96e64d809af.html" title="Edit section: References">edit</a>]</span> <span class="mw-headline">References</span></h2> 
      <ul> 
       <li>E.T. Jaynes, <a href="cfa23c35a836506c109a8779949da6c1.html" class="external text" title="http://bayes.wustl.edu/etj/articles/stand.on.entropy.pdf" rel="nofollow">Where do we stand on maximum entropy?</a> in: R. Levine, M. Tribus (Eds.), <i>The Maximum Entropy Formalism</i>, MIT Press, Cambridge, MA (1979). <a href="a34efe30ed94b68e82d3582f8149b87c.html" class="internal">ISBN 0-262-12080-1</a>.</li> 
       <li><a href="ac5bc4dce934caea3d30af537d51e05f.html" class="external text" title="http://bayes.wustl.edu/etj/node1.html" rel="nofollow">Extensive archive of further papers</a> by E.T. Jaynes on probability and physics. Many are collected in R.D. Rosenkrantz (Ed.), <i>E.T. Jaynes - Papers on probability, statistics and statistical physics</i>, D. Reidel, Dordrecht, (1983) <a href="9f9de255063b130566c65d7e5c05d08f.html" class="internal">ISBN 90-277-1448-7</a></li> 
       <li>S.F. Gull, <a href="f8d32f4d93fdfd2bae9ecb1c483b2892.html" class="external text" title="http://www.ucl.ac.uk/~ucesjph/reality/entropy/text.html" rel="nofollow">Some misconceptions about entropy</a> in: B. Buck, V. A. Macaulay (Eds.), <i>Maximum Entropy in Action</i>, OUP, Oxford (1991). <a href="137fa8bafb4e66d05f49ebd9ee549971.html" class="internal">ISBN 0-19-853963-0</a>.</li> 
       <li>Jochen Rau, <a href="d2cc0cedbbb3e7acd2e47062bacb9b66.html" class="external text" title="http://arxiv.org/abs/physics/9805024" rel="nofollow">Statistical Mechanics in a Nutshell</a> lecture notes (1998)</li> 
       <li>W.T. Grandy, <i>Foundations of Statistical Mechanics vol 1: Equilibrium Theory; vol 2: Nonequilibrium Phenomena</i>, D. Reidel, Dordrecht, (1987) <a href="e3c32f4706d88ce5e586fde5e1affa68.html" class="internal">ISBN 90-277-2489-X</a>, (1988) <a href="a82c22994e1a2f34c125f592abc85874.html" class="internal">ISBN 90-277-2649-3</a>.</li> 
       <li>W.T. Grandy, <a href="e0084a5ac021f44c0de1e5072917a650.html" class="external text" title="http://physics.uwyo.edu/~tgrandy/Statistical_Mechanics.html" rel="nofollow">Three papers in nonequilibrium statistical mechanics</a>, Found.Phys. <b>34</b>, 1 (2004); Found.Phys. <b>34</b>, 21 (2004); Found.Phys. <b>34</b>, 771 (2004)</li> 
       <li>R. Lorenz, <a href="5bd8fbdecd2e9869ab4eae29b0b3d653.html" class="external text" title="http://www.lpl.arizona.edu/~rlorenz/fullsteamahead.pdf" rel="nofollow">Full steam ahead - probably</a>, Science <b>299</b>, 837-838 (2003)</li> 
       <li>R.C. Dewar, <a href="f456bdcb562f944c9f7cdca60aa9f604.html" class="external text" title="http://arxiv.org/abs/cond-mat/0005382" rel="nofollow">Information theory explanation of the fluctuation theorem, maximum entropy production and self-organized criticality in non-equilibrium stationary states</a>, J. Phys. A: Math.Gen., <b>36</b> 631-641 (2003)</li> 
       <li>R.C. Dewar, <a href="c66e9bd51167a9d74d3073399abf754a.html" class="external text" title="http://www.iop.org/EJ/abstract/0305-4470/38/21/L01/" rel="nofollow">Maximum entropy production and the fluctuation theorem</a>, J. Phys. A: Math.Gen., <b>38</b> L371-L381 (2005)</li> 
       <li>G. Grinstein and R. Linsker, <a href="a69bf03cecf738bb9e85e7d5247ff57d.html" class="external text" title="http://www.iop.org/EJ/abstract/1751-8121/40/31/N01/" rel="nofollow">Comments on a derivation and application of the 'maximum entropy production' principle</a>, J. Phys. A: Math. Theor., <b>40</b> 9717-9720 (2007). Shows invalidity of Dewar's derivations (a) of maximum entropy production (MaxEP) from fluctuation theorem for far-from-equilibrium systems, and (b) of a claimed link between MaxEP and self-organized criticality.</li> 
      </ul> 
      <!-- 
NewPP limit report
Preprocessor node count: 91/1000000
Post-expand include size: 0/2048000 bytes
Template argument size: 0/2048000 bytes
Expensive parser function count: 0/500
--> 
      <div class="printfooter"> 
      </div> 
      <div id="catlinks">
       <div id="catlinks" class="catlinks">
        <div id="mw-normal-catlinks">
         <a href="8c137ecd26c0d26635e0b107a5f0c226.html" title="Special:Categories">Categories</a>: 
         <span dir="ltr"><a href="c84c752261b3f2981d5f3692760bd1fc.html" title="Category:Statistical mechanics">Statistical mechanics</a></span> | 
         <span dir="ltr"><a href="29b3247e522a5464c917af9fa108e3a3.html" title="Category:Philosophy of thermal and statistical physics">Philosophy of thermal and statistical physics</a></span> | 
         <span dir="ltr"><a href="c77aa05ab3b3fe1f8eaf3b59a9b4d199.html" title="Category:Non-equilibrium thermodynamics">Non-equilibrium thermodynamics</a></span> | 
         <span dir="ltr"><a href="a075b92f649cfb91aaf37fc070b60b2d.html" title="Category:Information theory">Information theory</a></span> | 
         <span dir="ltr"><a href="77144f3535c5b0384c22ad36e611ed07.html" title="Category:Thermodynamic entropy">Thermodynamic entropy</a></span>
        </div>
       </div>
      </div> 
      <!-- end content --> 
      <div class="visualClear"></div> 
     </div> 
    </div> 
   </div> 
   <div id="column-one"> 
    <div id="p-cactions" class="portlet"> 
     <h5>Views</h5> 
     <ul> 
      <li id="ca-nstab-main" class="selected"><a href="1d1acb969bd3c446bb3df222e3310e61.html">Article</a></li>
      <li id="ca-talk"><a href="37d31de5e992ef8aa070ee9560645d83.html">Discussion</a></li>
      <li id="ca-current"><a href="a269bea92536e5faecac85cba6405f91.html">Current revision</a></li> 
     </ul> 
    </div> 
    <div class="portlet" id="p-logo"> 
     <a style="background-image: url(../../../../misc/Wiki.png);" href="329da8a2279a712b4d91fcdf54d6b2f4.html" title="Main Page"></a> 
    </div> 
    <script type="text/javascript"> if (window.isMSIE55) fixalpha(); </script> 
    <div class="portlet" id="p-navigation"> 
     <h5>Navigation</h5> 
     <div class="pBody"> 
      <ul> 
       <li id="n-mainpage"><a href="d02efdfa1315ecddaa92c28836b98d09.html">Main Page</a></li> 
       <li id="n-Contents"><a href="06c5e1adf018b500bbd6b1da1f50c04c.html">Contents</a></li> 
       <li id="n-featuredcontent"><a href="a32429fc001fdee9ea8d8a8a467e0e8b.html">Featured content</a></li> 
       <li id="n-currentevents"><a href="320d486af916ae403f9611a8f973e912.html">Current events</a></li> 
      </ul> 
     </div> 
    </div> 
    <div class="portlet" id="p-interaction"> 
     <h5>Interaction</h5> 
     <div class="pBody"> 
      <ul> 
       <li id="n-aboutsite"><a href="8ee9ffe21d6358ccf09c83bfbce13eb4.html">About Wikipedia</a></li> 
       <li id="n-portal"><a href="bf5757cc5e44f17722bbe3d802ace082.html">Community portal</a></li> 
       <li id="n-recentchanges"><a href="d17d4f9d1d2de945d82ee9b8cca121bc.html">Recent changes</a></li> 
       <li id="n-contact"><a href="d60f80c59d5771086f3302bb0b11379b.html">Contact Wikipedia</a></li> 
       <li id="n-sitesupport"><a href="77ab72790b81b47c22dde0ec425a0948.html">Donate to Wikipedia</a></li> 
       <li id="n-help"><a href="e140d45e28ba328f9324aca72682a33b.html">Help</a></li> 
      </ul> 
     </div> 
    </div> 
    <div id="p-search" class="portlet"> 
     <h5><label for="searchInput">Search</label></h5> 
     <div id="searchBody" class="pBody"> 
      <form action="javascript:goToStatic(3)" id="searchform">
       <div> 
        <input id="searchInput" name="search" type="text" accesskey="f" value=""> 
        <input type="submit" name="go" class="searchButton" id="searchGoButton" value="Go"> 
       </div>
      </form> 
     </div> 
    </div> 
   </div>
   <!-- end of the left (by default at least) column --> 
   <div class="visualClear"></div> 
   <div id="footer"> 
    <div id="f-poweredbyico">
     <a href="e728e73850c6541823bfb73f732d84fb.html"><img src="../../../../skins/common/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki"></a>
    </div> 
    <div id="f-copyrightico">
     <a href="4e9a7ee49c9a8c8c9ca3e1cee44319e3.html"><img src="../../../../misc/wikimedia-button.png" border="0" alt="Wikimedia Foundation"></a>
    </div> 
    <ul id="f-list"> 
     <li id="f-credits">This page was last modified 18:48, 10 October 2007 by Wikipedia user Rl1rl1. Based on work by Wikipedia user(s) <a href="c47b95e5b68bf71f2757be03f1673c8d.html" title="User:Karada">Karada</a>, <a href="f74b72c1d5ec5ef0edd8968e603d67b2.html" title="User:Cydebot">Cydebot</a>, <a href="2db2ae3ccf450434ae8088a34f44b005.html" title="User:Jheald">Jheald</a>, <a href="381c9b2c59b8f29453e308e72be7d5f6.html" title="User:Dekimasu">Dekimasu</a>, <a href="9b1a15861218890422b27ce728d1453d.html" title="User:EdJohnston">EdJohnston</a>, <a href="58ee72d01a54cad57afa40bd998fc886.html" title="User:SmackBot">SmackBot</a>, <a href="3021cd4920958b2d90180a733fa8b6ad.html" title="User:Jeff3000">Jeff3000</a>, Jdthood, <a href="47cfcbcc82e654b36dc5fd23bd4d7e2e.html" title="User:Marra">Marra</a>, -k -J - IL, <a href="1d8890248b98df0497463f296311f16f.html" title="User:Shenme">Shenme</a>, <a href="1067d3bedddcc55ae50d51719410837e.html" title="User:HappyCamper">HappyCamper</a>, <a href="495d938e4b8e2e62ce4ad79a1a6c34d3.html" title="User:Conscious">Conscious</a>, <a href="7fc60cc026f1ac2064dae23fe3f703fa.html" title="User:Cyan">Cyan</a>, <a href="5b02292461de949eee39b5a1dab95a68.html" title="User:ABCD">ABCD</a> and <a href="62426f45c78998d8236ee96328063553.html" title="User:Denni">Denni</a> and Anonymous user(s) of Wikipedia.</li> 
     <li id="f-copyright">All text is available under the terms of the <a class="internal" href="ccadf67be90a334a07b6b66ce05aab58.html" title="Wikipedia:Text of the GNU Free Documentation License">GNU Free Documentation License</a>. (See <b><a class="internal" href="3b1e5f3786372a305ea987abd0d58264.html" title="Wikipedia:Copyrights">Copyrights</a></b> for details.) <br> WikipediaÂ® is a registered trademark of the <a href="5fee0adbcf57892d931dc3bf4805558a.html">Wikimedia Foundation, Inc</a>., a U.S. registered <a class="internal" href="c6feac284e644e935ba9801a5bc19f59.html" title="501(c)(3)">501(c)(3)</a> <a href="b6af017adf06cbea3b9a1e99989df532.html">tax-deductible</a> <a class="internal" href="d672d85d32d98ad30e393e54a4ad3349.html" title="Non-profit organization">nonprofit</a> <a href="6493772029328a4a34d49d8d9d5d8654.html" title="Charitable organization">charity</a>.<br></li> 
     <li id="f-about"><a href="02159763b4917dc9945833e6fd3ad96c.html" title="Wikipedia:About">About Wikipedia</a></li> 
     <li id="f-disclaimer"><a href="1043208a0ae41d2851aa75ff1abfe29d.html" title="Wikipedia:General disclaimer">Disclaimers</a></li> 
    </ul> 
   </div> 
  </div>   
 </body>
</html>
